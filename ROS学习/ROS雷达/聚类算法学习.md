# 欧式聚类分割

## pcl::EuclideanClusterExtraction

是基于欧式距离提取集群的方法，仅依据距离，将小于距离阈值的点云作为一个集群。

具体的实现方法大致是：

1. 找到空间中某点p10，由kdTree找到离他最近的n个点，判断这n个点到p的距离；
2. 将距离小于阈值r的点p12、p13、p14…放在类Q里；
3. 在 Q\p10 里找到一点p12，重复1；
4. 在 Q\p10、p12 找到一点，重复1，找到p22、p23、p24…全部放进Q里；
5. 当 Q 再也不能有新点加入了，则完成搜索了。

## Fast Euclidean Clustering(FEC)

https://www.mdpi.com/2504-446X/6/11/325

https://github.com/unageek/fast-euclidean-clustering

# 划分类聚类方法

## K-means

### 工作原理

K-means是一种基于划分的聚类方法，它的基本思想是将数据集分为K个簇，使得簇内的数据点尽可能地相似，簇与簇之间尽可能不同。其具体步骤如下：

1. 随机/按照特定算法选择K个初始中心点。
2. 将每个数据点分配给最近的中心点，形成K个簇。
3. 对每个簇，重新计算簇内所有点的均值，将均值作为新的簇中心。
4. 重复步骤2和3，直到簇的分配不再变化（收敛）。

### 优点：

- 简单易理解。
- 收敛速度较快，适合大规模数据集。
- 算法实现相对简单。

### 缺点：

- 需要预先指定K值，可能不容易确定最佳K值。
- 对初始中心点的选择非常敏感，可能导致局部最优解。
- 对噪声和异常值敏感。
- 不适合处理非球形簇。

## K-means++

### 工作原理：

K-means++是对K-means算法初始化步骤的改进，旨在通过更加智能地选择初始中心点来提高K-means的性能和稳定性。其具体步骤如下：

1. 随机选择一个数据点作为第一个中心点。
2. 对于剩余的数据点，计算它们与已选中心点的最小距离，并选择距离较远的数据点作为新的中心点。
3. 重复步骤2，直到选择出K个中心点。
4. 进行K-means聚类算法，直到收敛。

### 优点：

- 相比K-means，K-means++在选择初始中心时更加均匀分布，减少了局部最优的可能性。
- 提高了算法的稳定性和收敛速度。
- 结果通常比K-means更加可靠。

### 缺点：

- 计算复杂度较高，尤其是在数据量较大的时候。

## bi-kmeans

### 工作原理：

Bi-Kmeans是K-means的一个变种，它将数据集分为两个子簇，而不是直接对所有数据进行K个簇的划分。其基本步骤如下：

1. 将数据集分成两个簇。
2. 对每个簇应用K-means算法，将簇进一步分成子簇。
3. 重复此过程，直到达到所需的簇数。

这种方法通过递归的方式不断将簇细分，直到所有的簇都满足要求。

### 优点：

- 适合数据集簇形不规则的情况，比标准K-means能够获得更好的结果。
- 通过递归分割，能够更好地捕捉簇的结构。

### 缺点：

- 计算开销较大，因为每次都需要对簇进行细分和迭代。
- 可能产生过拟合，导致划分过细。

# 层次聚类算法

层次聚类是一种基于树结构的聚类方法，通常分为自底向上的聚合方法（agglomerative）和自顶向下的分割方法（divisive）。

# 基于密度的聚类方法

## DBSCAN

### 工作原理：

DBSCAN是一种基于密度的聚类方法，它通过寻找具有较高密度区域的数据点来识别簇，并将密度较低的区域视为噪声。其主要思想是：

1. 对于每个点，计算其领域内的邻居点数量，如果该点的邻居点数量超过设定的最小点数（MinPts）并且与邻居点的距离小于设定的ε（邻域大小），则该点是核心点。
2. 核心点会形成一个簇，边界点是邻域内有一些点但不足以构成核心点的点，而噪声点是那些既不是核心点也不是边界点的点。
3. 通过将核心点相连来扩展簇，直到所有点都被标记为簇的一部分或噪声。

### 优点：

- 不需要预先指定簇数。
- 能够发现任意形状的簇。
- 能有效处理噪声数据，能够识别稀疏区域。
- 对非球形簇的处理效果好。

### 缺点：

- 对参数ε（邻域大小）和MinPts（最小点数）比较敏感，选择不当可能导致结果不理想。
- 对数据的密度差异较为敏感，密度变化较大的数据集可能难以处理。
